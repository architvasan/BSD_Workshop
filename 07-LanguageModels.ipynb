{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63629c23",
   "metadata": {
    "id": "J1tOS7oWba4s"
   },
   "source": [
    "# Language models (LMs)\n",
    "\n",
    "Author: Archit Vasan , including materials on LLMs by Varuni Sastri and Carlo Graziani at Argonne, and discussion/editorial work by Taylor Childers, Bethany Lusch, and Venkat Vishwanath (Argonne)\n",
    "\n",
    "Inspiration from the blog posts \"The Illustrated Transformer\" and \"The Illustrated GPT2\" by Jay Alammar, highly recommended reading.\n",
    "\n",
    "Although the name \"language models\" is derived from Natural Language Processing, the models used in these approaches can be applied to diverse scientific applications as illustrated below. \n",
    "\n",
    "This session is dedicated to setting out the basics of sequential data modeling, and introducing a few key elements required for DL approaches to such modeling---principally Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e34d3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "During this session I will cover:\n",
    "1. Scientific applications modeling sequential data\n",
    "2. Brief History of Language Models\n",
    "3. Tokenization and embedding of sequential data\n",
    "5. Elements of a Transformer\n",
    "6. Attention mechanisms\n",
    "7. Output layers\n",
    "8. Training loops\n",
    "9. Different types of Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008761de",
   "metadata": {},
   "source": [
    "## Modeling Sequential Data\n",
    "\n",
    "Sequences are variable-length lists with data in subsequent iterations that depends on previous iterations (or tokens).\n",
    "\n",
    "Mathematically: \n",
    "A sequence is a list of tokens: $$T = [t_1, t_2, t_3,...,t_N]$$ where each token within the list depends on the others with a particular probability:\n",
    "\n",
    "$$P(t_2 | t_1, t_3, t_4, ..., t_N)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c38c2",
   "metadata": {},
   "source": [
    "The purpose of sequential modeling is to learn these probabilities for possible tokens in a distribution to perform various tasks including:\n",
    "* Sequence generation based on a prompt\n",
    "* Language translation (e.g. English --> French)\n",
    "* Property prediction (predicting a property based on an entire sequence)\n",
    "* Identifying mistakes or missing elements in sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a2add4",
   "metadata": {},
   "source": [
    "## Scientific sequential data modeling examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf4397f",
   "metadata": {},
   "source": [
    " ### Nucleic acid sequences + genomic data\n",
    "Nucleic acid sequences can be used to predict translation of proteins, mutations, and gene expression levels.\n",
    " ![RNA sequences](images/RNA-codons.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb9a33",
   "metadata": {},
   "source": [
    "Here is an image of GenSLM. This is a language model developed by Argonne researchers that can model genomic information in a single model. It was shown to model the evolution of SARS-COV2 without expensive experiments.\n",
    "\n",
    "![GenSLM](images/genslm.png)\n",
    "Image credit: Zvyagin et. al 2022. BioRXiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932e427",
   "metadata": {},
   "source": [
    "### Protein sequences\n",
    "Protein sequences can be used to predict folding structure, protein-protein interactions, chemical/binding properties, protein function and many more properties.\n",
    "![Protein sequences](images/Protein-Structure-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4922ee5",
   "metadata": {},
   "source": [
    "### Other applications:\n",
    "\n",
    "* Biomedical text\n",
    "* SMILES strings\n",
    "* Weather predictions\n",
    "* Interfacing with simulations such as molecular dynamics simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68af144",
   "metadata": {
    "id": "XmbDaGypbmRE"
   },
   "source": [
    "## Overview of Language models\n",
    "\n",
    "We will now briefly talk about the progression of language models.\n",
    "\n",
    "### RNNs\n",
    "\n",
    "Recurrent Neural Newtorks(RNNs) were a traditional model used to determine temporal dependencies within data. \n",
    "\n",
    "In RNNs, the hidden state from the previous time step is fed back into the network, allowing it to maintain a “memory” of past inputs. \n",
    "\n",
    "They were ideal for tasks with short sequences such as natural language processing and time-series prediction.\n",
    "\n",
    "![RNN](images/recurrent_nn.png)\n",
    "\n",
    "However, these networks had significant challenges.\n",
    "\n",
    "* Slow to train: \n",
    "    RNNs process data one element at a time, maintaining an internal hidden state that is updated at each step. They operate recurrently, where each output depends on the previous hidden state and the current input; thus, parallel computation is not possible.\n",
    "* Cannot handle large sequences: \n",
    "    Exploding and vanishing gradients limit the RNN modelling of long sequences. Some variants of RNNs such as LSTM and GRU addressed this problem, they cannot engage with very large sequences.\n",
    "* Fixed vector length: \n",
    "    Limits the representation and decodinng of a long input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed3544",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "The newest LMs referred to as \"large language models\" (since they have large parameter size) were developed to address many of these challenges. \n",
    "\n",
    "These new models base their desin on the Transformer architecture that was introduced in 2017 in the \"Attention is all you need\" paper. \n",
    "\n",
    "Since then a multitude of LLM architectures have been designed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cbae43",
   "metadata": {
    "id": "spruqgx1qI06"
   },
   "source": [
    "![en_chapter1_transformers_chrono.svg](images/en_chapter1_transformers_chrono.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861648a",
   "metadata": {
    "id": "D7PCbnnt4STj"
   },
   "source": [
    "Image credit: https://huggingface.co/learn/nlp-course/chapter1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a129803",
   "metadata": {},
   "source": [
    "The power of these models comes from the \"attention mechanism\" defined in the Vaswani 2017 seminal paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35fd09",
   "metadata": {
    "id": "zkgYJa2BqTO6"
   },
   "source": [
    "![Transformer_Arch.png](images/attention_is_all_you_need.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046bcecb",
   "metadata": {},
   "source": [
    "## Coding example of LLMs in action!\n",
    "\n",
    "Let's look at an example of running inference with a LLM as a block box to generate text given a prompt and we will also initiate a training loop for an LLM:\n",
    "\n",
    "Here, we will use the `transformers` library which is as part of HuggingFace, a repository of different models, tokenizers and information on how to apply these models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c654426",
   "metadata": {},
   "source": [
    "*Warning: Large Language Models are only as good as their training data. They have no ethics, no judgement, or editing ability. We will be using some pretrained models from Hugging Face which used wide samples of internet hosted text. The datasets have not been strictly filtered to restrict all malign content so the generated text may be surprisingly dark or questionable. They do not reflect our core values and are only used for demonstration purposes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b4675",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM, AutoConfig\n",
    "input_text = \"My dog really wanted to\"\n",
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
    "generator(input_text, max_length=20, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15bb26",
   "metadata": {},
   "source": [
    "We can also train a language model given input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128) \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset('dataset/train_input.txt','dataset/test_input.txt', tokenizer)\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    eval_steps = 40, # Number of update steps between two evaluations.\n",
    "    save_steps=80, # after # steps model is saved \n",
    "    warmup_steps=50,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1962543",
   "metadata": {},
   "source": [
    "## What's going on under the hood?\n",
    "There are two components that are \"black-boxes\" here:\n",
    "\n",
    "1. The method for tokenization\n",
    "2. The model that generates novel text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7ef89",
   "metadata": {
    "id": "SPnOYsJHkWc5"
   },
   "source": [
    "## Tokenization and embedding of sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56661b4",
   "metadata": {
    "id": "MjLqoEJTtWku"
   },
   "source": [
    "Humans can inherently understand language data because they previously learned phonetic sounds. Machines don’t have phonetic knowledge so they need to be told how to break text into standard units to process it.\n",
    "They use a system called “tokenization”, where sequences of text are broken into smaller parts, or “tokens”, and then fed as input.\n",
    "\n",
    "![text-processing---machines-vs-humans.png](images/text-processing---machines-vs-humans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14618043",
   "metadata": {
    "id": "DF8jQqv2Iqa_"
   },
   "source": [
    "Image credit: https://blog.floydhub.com/tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43344e8",
   "metadata": {
    "id": "HYjPS-FptjI0"
   },
   "source": [
    "### Tokenizing based on \"words\"\n",
    "\n",
    "Based on syntax of English language a likely answer is just that breaking sentences into word-level chunks or tokens seems like the best approach.\n",
    "\n",
    "Although this seems easy, it can actually be done in different ways as shown in the following diagram.\n",
    "\n",
    "![tokenize_words.png](images/tokenize_words.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e37e2f",
   "metadata": {
    "id": "gqT-1stEIvJ8"
   },
   "source": [
    "Image credit: https://blog.floydhub.com/tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac1321",
   "metadata": {
    "id": "nVKYssvMH7tU"
   },
   "source": [
    "There are some issues with this approach though:\n",
    "\n",
    "* You need a big vocabulary: You can only learn those words in your training vocab. Any words not in the training set will be treated as unknown words. It does not break words into sub-words so it would miss anything like “talk” vs. “talks” vs. “talked” and “talking”.\n",
    "* Words are combined: There may be some confusion about what exactly constitutes a word. Some words such as “sun” and “flower” are compounded to make sunflower. Are these one word or multiple?\n",
    "* Some languages don’t segment by spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2287be8d",
   "metadata": {
    "id": "D5qZKrpvJAlK"
   },
   "source": [
    "### Character-based tokenization\n",
    "\n",
    "To potentially solve this we can try to simply tokenize the input text character by character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d47615c",
   "metadata": {
    "id": "x_jsrK_mtrJN"
   },
   "source": [
    "![chars-tokenization.png](images/chars-tokenization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723ebf3",
   "metadata": {
    "id": "6ITNMqvdJpPC"
   },
   "source": [
    "Image credit: https://blog.floydhub.com/tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29376b",
   "metadata": {
    "id": "qQUK4D67KJKr"
   },
   "source": [
    "Issues with this approach:\n",
    "* Lack of meaning: Unlike words, characters don’t have any inherent meaning, so there is no guarantee that the resultant learned representations will have any meaning.\n",
    "* Increased input computation: If you use word level tokens then you will spike a 7-word sentence into 7 input tokens. However, assuming an average of 5 letters per word (in the English language) you now have 35 inputs to process. This increases the complexity of the scale of the inputs you need to process\n",
    "* Limits network choices: Increasing the size of your input sequences at the character level also limits the type of neural networks you can use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec723c",
   "metadata": {
    "id": "T0HJjUTpt2K7"
   },
   "source": [
    "### Subword tokenization\n",
    "This tokenization type deals with an infinite potential vocabulary via a finite list of known words.\n",
    "\n",
    "There are different ways of doing this:\n",
    "\n",
    "**Byte-pair encoding**\n",
    "\n",
    "![Byte_Pair_enc.webp](images/Byte_Pair_enc.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c6aa51",
   "metadata": {
    "id": "eojXMQljL9yk"
   },
   "source": [
    "Image credit: https://towardsdatascience.com/tokenization-algorithms-explained-e25d5f4322ac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c835f225",
   "metadata": {
    "id": "4Ciy9s6wLWso"
   },
   "source": [
    "BPE was initially introduced to help compress data by finding common byte pair combinations.\n",
    "\n",
    "This tokenization first forms a base vocabulary which is a collection of all unique characters present in the corpus. We also calculate frequency of each token and represent each token as a list of individual characters from base vocabulary.\n",
    "\n",
    "Now merging begins. We keep adding tokens to our base vocab as long as the maximum size is not breached on the basis of following criteria — the pair of tokens occurring most number of times is merged and introduced as a new token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704323d",
   "metadata": {
    "id": "dnazXlhpMY7p"
   },
   "source": [
    "**Word-piece tokenizer**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c1585",
   "metadata": {
    "id": "-tOHGsXIuBxb"
   },
   "source": [
    "Word-piece tokenization is similar to BPE but instead maximizes the likelihood of token pairs:\n",
    "\n",
    "![WordPieceTok.webp](images/WordPieceTok.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d30aa",
   "metadata": {
    "id": "p4N8O45PMvJR"
   },
   "source": [
    "Word-piece and BPE will go through every potential option at each step and pick the tokens to merge based on the highest frequency/likelihood. In this way it is a greedy algorithm which optimizes for the best solution at each step in its iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc50724",
   "metadata": {},
   "source": [
    "### Example of tokenization\n",
    "Let's look at an example of tokenization using byte-pair encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# A utility function to tokenize a sequence and print out some information about it.\n",
    "\n",
    "def tokenization_summary(tokenizer, sequence):\n",
    "\n",
    "    # get the vocabulary\n",
    "    vocab = tokenizer.vocab\n",
    "    # Number of entries to print\n",
    "    n = 10\n",
    "\n",
    "    # Print subset of the vocabulary\n",
    "    print(\"Subset of tokenizer.vocab:\")\n",
    "    for i, (token, index) in enumerate(tokenizer.vocab.items()):\n",
    "        print(f\"{token}: {index}\")\n",
    "        if i >= n - 1:\n",
    "            break\n",
    "\n",
    "    print(\"Vocab size of the tokenizer = \", len(vocab))\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    # .tokenize chunks the existing sequence into different tokens based on the rules and vocab of the tokenizer.\n",
    "    tokens = tokenizer.tokenize(sequence)\n",
    "    print(\"Tokens : \", tokens)\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "    # .convert_tokens_to_ids or .encode or .tokenize converts the tokens to their corresponding numerical representation.\n",
    "    #  .convert_tokens_to_ids has a 1-1 mapping between tokens and numerical representation\n",
    "    # ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    # print(\"encoded Ids: \", ids)\n",
    "\n",
    "    # .encode also adds additional information like Start of sequence tokens and End of sequene\n",
    "    print(\"tokenized sequence : \", tokenizer.encode(sequence))\n",
    "\n",
    "    # .tokenizer has additional information about attention_mask.\n",
    "    # encode = tokenizer(sequence)\n",
    "    # print(\"Encode sequence : \", encode)\n",
    "    # print(\"------------------------------------------\")\n",
    "\n",
    "    # .decode decodes the ids to raw text\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    decode = tokenizer.decode(ids)\n",
    "    print(\"Decode sequence : \", decode)\n",
    "\n",
    "\n",
    "tokenizer_1  =  AutoTokenizer.from_pretrained(\"gpt2\") # GPT-2 uses \"Byte-Pair Encoding (BPE)\"\n",
    "\n",
    "sequence = \"Counselor, please adjust your Zoom filter to appear as a human, rather than as a cat\"\n",
    "\n",
    "tokenization_summary(tokenizer_1, sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d7647",
   "metadata": {},
   "source": [
    "### Token embedding:\n",
    "\n",
    "Words are turned into vectors based on their location within a vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104de88",
   "metadata": {},
   "source": [
    "The strategy of choice for learning language structure from tokenized text is to find a clever way to map each token into a moderate-dimension vector space, adjusting the mapping so that\n",
    "\n",
    "Similar, or associated tokens take up residence nearby each other, and different regions of the space correspond to different position in the sequence.\n",
    "Such a mapping from token ID to a point in a vector space is called a token embedding. The dimension of the vector space is often high (e.g. 1024-dimensional), but much smaller than the vocabulary size (30,000--500,000). \n",
    "\n",
    "Various approaches have been attempted for generating such embeddings, including static algorithms that operate on a corpus of tokenized data as preprocessors for NLP tasks. Transformers, however, adjust their embeddings during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcbf867",
   "metadata": {},
   "source": [
    "### We can visualize these embeddings of the popular BERT model using t-SNE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e69c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "plt.rcParams['figure.figsize'] = [100, 60]\n",
    "\n",
    "# Load BERT.\n",
    "model = BertModel.from_pretrained('bert-large-uncased-whole-word-masking')\n",
    "# Set the model to eval mode.\n",
    "model.eval()\n",
    "# This notebook assumes CPU execution. If you want to use GPUs, put the model on cuda and modify subsequent code blocks.\n",
    "#model.to('cuda')\n",
    "# Load tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking')\n",
    "\n",
    "# Save the BERT vocabulary to a file -- by default it will name this file \"vocab.txt\".\n",
    "tokenizer.save_vocabulary(save_directory='.')\n",
    "\n",
    "print(\"The vocabulary size is: \", model.config.vocab_size) # Size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec196ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BERT's vocabulary embeddings.\n",
    "wordembs = model.get_input_embeddings()\n",
    "\n",
    "# Convert the vocabulary embeddings to numpy.\n",
    "allinds = np.arange(0,model.config.vocab_size,1)\n",
    "inputinds = torch.LongTensor(allinds)\n",
    "bertwordembs = wordembs(inputinds).detach().numpy()\n",
    "print(bertwordembs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcd33be",
   "metadata": {},
   "source": [
    "The visualization of this data will be carried out by means of a projection to 2 dimensions by an algorithm called \"t-SNE\" (\"t-distributed stochastic neighbor embedding\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0056cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the vocabulary\n",
    "filename = \"vocab.txt\"\n",
    "with open(filename,'r') as f:\n",
    "    bertwords = np.array([])\n",
    "    for line in f:\n",
    "        bertwords = np.append(bertwords, line.rstrip())\n",
    "\n",
    "# Determine vocabulary to use for t-SNE/visualization. The indices are hard-coded based partially on inspection:\n",
    "bert_char_indices_to_use = np.arange(999, 1063, 1)\n",
    "bert_voc_indices_to_plot = np.append(bert_char_indices_to_use, np.arange(1996, 5932, 1))\n",
    "bert_voc_indices_to_use = np.append(bert_char_indices_to_use, np.arange(1996, 11932, 1))\n",
    "\n",
    "bert_voc_indices_to_use_tensor = torch.LongTensor(bert_voc_indices_to_use)\n",
    "bert_word_embs_to_use = wordembs(bert_voc_indices_to_use_tensor).detach().numpy()\n",
    "bert_words_to_plot = bertwords[bert_voc_indices_to_plot]\n",
    "\n",
    "\n",
    "print(len(bert_voc_indices_to_plot))\n",
    "print(len(bert_voc_indices_to_use))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0061088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run t-SNE on the BERT vocabulary embeddings we selected:\n",
    "mytsne_words = TSNE(n_components=2,early_exaggeration=12,verbose=2,metric='cosine',init='pca',n_iter=2500)\n",
    "bert_word_embs_to_use_tsne = mytsne_words.fit_transform(bert_word_embs_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd4f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the transformed BERT vocabulary embeddings:\n",
    "fig = plt.figure()\n",
    "alltexts = list()\n",
    "for i, txt in enumerate(bert_words_to_plot):\n",
    "    plt.scatter(bert_word_embs_to_use_tsne[i,0], bert_word_embs_to_use_tsne[i,1], s=0)\n",
    "    currtext = plt.text(bert_word_embs_to_use_tsne[i,0], bert_word_embs_to_use_tsne[i,1], txt, family='sans-serif')\n",
    "    alltexts.append(currtext)\n",
    "    \n",
    "\n",
    "# Save the plot before adjusting.\n",
    "plt.savefig('viz-bert-voc-tsne10k-viz4k-noadj.pdf', format='pdf')\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be8cb4",
   "metadata": {},
   "source": [
    "You should see common words/subwords grouped together!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb270682",
   "metadata": {},
   "source": [
    "## Elements of a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4681d",
   "metadata": {},
   "source": [
    "Now let's look at the base elements that make up a Transformer by dissecting the popular GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d151140",
   "metadata": {
    "id": "d82IczeVoAuV"
   },
   "source": [
    "GPT2 is an example of a Transformer Decoder which is used to generate novel text. \n",
    "\n",
    "Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.\n",
    "\n",
    "The pretraining of decoder models usually revolves around predicting the next word in the sentence.\n",
    "\n",
    "These models are best suited for tasks involving text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d1031",
   "metadata": {
    "id": "V4PcUAAhoA2H"
   },
   "source": [
    "Examples of these include:\n",
    "* CTRL\n",
    "* GPT\n",
    "* GPT-2\n",
    "* Transformer XL\n",
    "\n",
    "Let's discuss one of the most popular models, GPT-2 in a little more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022814c",
   "metadata": {
    "id": "HdGPhHW9Pm0U"
   },
   "source": [
    "The architecture of GPT-2 is inspired by the paper: \"Generating Wikipedia by Summarizing Long Sequences\" which is another arrangement of the transformer block that can do language modeling. This model threw away the encoder and thus is known as the “Transformer-Decoder”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52460678",
   "metadata": {
    "id": "cig2mvfguetQ"
   },
   "source": [
    "![transformer-decoder-intro.png](images/transformer-decoder-intro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f73f86",
   "metadata": {
    "id": "S2jNiUvYcZUX"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c357a5",
   "metadata": {},
   "source": [
    "The Transformer-Decoder is composed of Decoder blocks stacked ontop of each other where each contains two types of layers: \n",
    "1. Masked Self-Attention and \n",
    "2. Feed Forward Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d681d",
   "metadata": {},
   "source": [
    "In this lecture, we will \n",
    "* First, discuss attention mechanisms at length as this is arguably the greatest contribution by Transformers.\n",
    "* Second, extend the discussion from last week (https://github.com/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/Sequential_Data_Models.ipynb) on embedding input data while taking into account position.\n",
    "* Third, discuss outputting real text/sequences from the models.\n",
    "* Fourth, build a training loop for a mini-LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4 ## so head_size = 16\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f70c6da",
   "metadata": {
    "id": "BowLYFlCrDrr"
   },
   "source": [
    "## Attention mechanisms\n",
    "\n",
    "Suppose the following sentence is an input sentence we want to translate using an LLM:\n",
    "\n",
    "`”The animal didn't cross the street because it was too tired”`\n",
    "\n",
    "Earlier, we mentioned that the Transformer learns an embedding of all words allowing interpretation of meanings of words.\n",
    "\n",
    "<img src=\"images/viz-bert-voc-verbs.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "So, if the model did a good job in token embedding, it will \"know\" what all the words in this sentence mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3428aede",
   "metadata": {},
   "source": [
    "But to understand a full sentence, the model also need to understand what each word means in relation to other words.\n",
    "\n",
    "For example, when we read the sentence:\n",
    "`”The animal didn't cross the street because it was too tired”`\n",
    "we know intuitively that the word `\"it\"` refers to `\"animal\"`, the state for `\"it\"` is `\"tired\"`, and the associated action is `\"didn't cross\"`.\n",
    "\n",
    "However, the model needs a way to learn all of this information in a simple yet generalizable way.\n",
    "What makes Transformers particularly powerful compared to earlier sequential architectures is how it encodes context with the **self-attention mechanism**.\n",
    "\n",
    "As the model processes each word in the input sequence, attention looks at other positions in the input sequence for clues to a better understanding for this word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56a72e",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer_self-attention_visualization.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caeb960",
   "metadata": {
    "id": "UGbAi0cJ7x3a"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484491fb",
   "metadata": {},
   "source": [
    "Self-attention mechanisms use 3 vectors to encode the context of a word in a sequence with another word:\n",
    "1. Query: the word representation we score other words against using the other word's keys\n",
    "2. Key: labels for the words in a sequence that we match against the query\n",
    "3. Value: actual word representation. We will use the queries and keys to score the word's relevance to the query, and multiply this by the value. \n",
    "\n",
    "An analogy provided by Jay Alammar is thinking about attention as choosing a file from a file cabinet according to information on a post-it note. You can use the post-it note (query) to identify the folder (key) that most matches the topic you are looking up. Then you access the contents of the file (value) according to its relevance to your query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94625260",
   "metadata": {},
   "source": [
    "<img src=\"images/self-attention-example-folders-3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e6f50e",
   "metadata": {},
   "source": [
    "In our models, we can encode queries, keys, and values using simple linear layers with the same size (`sequence length, head_size`). During the training process, these layers will be updated to best encode context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 32 # channels\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be9650",
   "metadata": {
    "id": "Jzf9VE_AqWeR"
   },
   "source": [
    "The algorithm for self-attention is as follows:\n",
    "\n",
    "1. Generate query, key and value vectors for each word\n",
    "2. Calculate a score for each word in the input sentence against each other.\n",
    "3. Divide the scores by the square root of the dimension of the key vectors to stabilize the gradients. This is then passed through a softmax operation.\n",
    "4. Multiply each value vector by the softmax score.\n",
    "5. Sum up the weighted value vectors to produce the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc744582",
   "metadata": {},
   "source": [
    "<img src=\"images/self-attention-output.png\" alt=\"Drawing\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcc2c04",
   "metadata": {
    "id": "yOwm-NkXA8U3"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791123c9",
   "metadata": {},
   "source": [
    "Let's see how attention is performed in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80373fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Here we want the wei to be data dependent - ie gather info from the past but in a data dependant way\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16) # each token here (totally B*T) produce a key and query in parallel and independently\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x)\n",
    "\n",
    "wei =  q @ k.transpose(-2, -1) * head_size**-0.5 # (B, T, 16) @ (B, 16, T) ---> (B, T, T). #\n",
    "wei = F.softmax(wei, dim=-1) # exponentiate and normalize giving a nice distibution that sums to 1 and\n",
    "                             # now it tells us that in a data dependent manner how much of info to aggregate from\n",
    "\n",
    "out = wei @ v # aggregate the attention scores and value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faee830",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8086552e",
   "metadata": {
    "id": "4lwyFlxKW6oA"
   },
   "source": [
    "### Multi-head attention\n",
    "\n",
    "In practice, multiple attention heads are used which\n",
    "1. Expands the model’s ability to focus on different positions and prevent the attention to be dominated by the word itself.\n",
    "2. Have multiple “representation subspaces”. Have multiple sets of Query/Key/Value weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caf3770",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer_multi-headed_self-attention-recap.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f1d4bc",
   "metadata": {
    "id": "6oHsezdVBIaf"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46bfcb",
   "metadata": {},
   "source": [
    "### Let's see attention mechanisms in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52f118",
   "metadata": {},
   "source": [
    "We are going to use the powerful visualization tool bertviz, which allows an interactive experience of the attention mechanisms. Normally these mechanisms are abstracted away but this will allow us to inspect our model in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4defaf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4ef46",
   "metadata": {},
   "source": [
    "Let's load in the model, GPT2 and look at the attention mechanisms. \n",
    "\n",
    "**Hint... click on the different blocks in the visualization to see the attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7020f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils, AutoModelForCausalLM\n",
    "\n",
    "from bertviz import model_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "model_name = 'openai-community/gpt2'\n",
    "input_text = \"No, I am your father\"  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "outputs = model(inputs)  # Run model\n",
    "attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\n",
    "model_view(attention, tokens)  # Display model view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e129c64",
   "metadata": {},
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd0a86",
   "metadata": {
    "id": "IFq78-kjbrWp"
   },
   "source": [
    "We just discussed attention mechanisms which account for context between words. Another question we should ask is how do we account for the order of words in an input sentence\n",
    "\n",
    "Consider the following two sentences to see why this is important:\n",
    "\n",
    "``The man ate the sandwich.``\n",
    "\n",
    "``The sandwich ate the man.``\n",
    "\n",
    "Clearly, these are two vastly different situations even though they have the same words. The Transformer can \n",
    "\n",
    "Transformers differentiate between these situations by adding a **Positional encoding** vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f39887",
   "metadata": {},
   "source": [
    "<img src=\"images/positional_encoding.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "Image credit: https://medium.com/@xuer.chen.human/llm-study-notes-positional-encoding-0639a1002ec0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da64264",
   "metadata": {},
   "source": [
    "We set up positional encoding similarly as token embedding using the ``nn.Embedding`` tool. We use a simple embedding here but there are more complex positional encodings used such as sinusoidal. \n",
    "\n",
    "For an explanation of different positional encodings, refer to this post: https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797391c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 65\n",
    "n_embd = 64\n",
    "\n",
    "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "position_embedding_table = nn.Embedding(block_size, n_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c73e4d1",
   "metadata": {},
   "source": [
    "You will notice the positional encoding size is `(block_size, n_embed)` because it encodes for the postion of a token within the sequence of size `block_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0d52d9",
   "metadata": {},
   "source": [
    "Then, the position embedding used is simply added to the token embedding to apply positional embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa3780",
   "metadata": {},
   "source": [
    "Let's look at token embedding alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,3,15,4,7,1,4,9])\n",
    "x = token_embedding_table(x)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57689539",
   "metadata": {},
   "source": [
    "And token + positional embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,3,15,4,7,1,4,9])\n",
    "x= position_embedding_table(x) + token_embedding_table(x)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acead6c",
   "metadata": {},
   "source": [
    "You can see a clear offset between these two embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc64817",
   "metadata": {},
   "source": [
    "During the training process, these embeddings will be learned to best encode the token and positional embeddings of the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f13a3c",
   "metadata": {
    "id": "iF1HzH9xNJ7S"
   },
   "source": [
    "## Output layers\n",
    "\n",
    "At the end of our Transformer model, we are left with a vector, so how do we turn this into a word?\n",
    "\n",
    "<img src=\"images/transformer-decoder-intro.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Using a final Linear layer and a Softmax Layer.\n",
    "The Linear layer projects the vector produced by the stack of decoders, into a larger vector called a logits vector.\n",
    "\n",
    "If our model knows 10,000 unique English words learned from its training dataset the logits vector is 10,000 cells wide – each cell corresponds to the score of a unique word.\n",
    "\n",
    "The softmax layer turns those scores into probabilities. The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382f692",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer_decoder_output_softmax.png\" alt=\"Drawing\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43abf272",
   "metadata": {
    "id": "HS6r-z8dN_RV"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e7180",
   "metadata": {
    "id": "XK8q67P03yr4"
   },
   "source": [
    "## Training\n",
    "\n",
    "How does an LLM improve over time?\n",
    "We want to compare the probabilitiy distribution for each token generated by our model to the ground truths. \n",
    "Our model produces a probability distribution for each token. We want to compare these probability distributions to the ground truths. \n",
    "For example, when translating the sentence: “je suis étudiant” into “i am a student” as can be seen in the example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e61247c",
   "metadata": {},
   "source": [
    "<img src=\"images/output_target_probability_distributions.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee0698",
   "metadata": {
    "id": "HS6r-z8dN_RV"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99737a1",
   "metadata": {},
   "source": [
    "The model can calculate the loss between the vector it generates and the ground truth vector seen in this example. A commonly used loss function is cross entropy loss:\n",
    "\n",
    "$CE = -\\sum_{x \\in X} p(x) log q(x)$\n",
    "\n",
    "where p(x) represents the true distribution and q(x) represents the predicted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac0937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "logits = torch.tensor([0.5, 0.1, 0.3])\n",
    "targets = torch.tensor([1.0, 0.0, 0.0])\n",
    "loss = F.cross_entropy(logits, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8207e4c1",
   "metadata": {},
   "source": [
    "Another important metric commonly used in LLMs is **perplexity**.\n",
    "\n",
    "Intuitively, perplexity means to be surprised. We measure how much the model is surprised by seeing new data. The lower the perplexity, the better the training is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f67b01",
   "metadata": {},
   "source": [
    "Mathematically, perplexity is just the exponent of the negative cross entropy loss:\n",
    "\n",
    "$\\text{perplexity} = exp(\\text{CE})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe97630",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb58235",
   "metadata": {},
   "source": [
    "## Let's train a mini-LLM from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b020d011",
   "metadata": {},
   "source": [
    "### Set up hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4 ## so head_size = 16\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa856db",
   "metadata": {},
   "source": [
    "### Load in data and create train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e53ede",
   "metadata": {},
   "source": [
    "We're going to be using the tiny Shakespeare dataset. \n",
    "Data is tokenized according to a simple character based tokenizer.\n",
    "Data is split into a train and test set so we have something to test after performing training (9:1 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee436b",
   "metadata": {},
   "source": [
    "### Set up the components of the Decoder block: \n",
    "* MultiHeadAttention\n",
    "* FeedForward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ba89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C) 16,32,16\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # Projection layer going back into the residual pathway\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e11c6f",
   "metadata": {},
   "source": [
    "### Combine components into the Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31757926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))    # Communication\n",
    "        x = x + self.ffwd(self.ln2(x))  # Computation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526a054a",
   "metadata": {},
   "source": [
    "### Set up the full Transformer model \n",
    "This is a combination of the Token embeddings, Positional embeddings, a stack of Transformer blocks and an output block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c9836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple language model\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96629f70",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "1. In this notebook, we learned the various components of an LLM. \n",
    "    Your homework this week is to take the mini LLM we created from scratch and run your own training loop. Show how the training and validation perplexity change over the steps.\n",
    "      \n",
    "    Hint: this function might be useful for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b47bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1be5c",
   "metadata": {},
   "source": [
    "## Different types of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c53af",
   "metadata": {},
   "source": [
    "### Encoder-Decoder architecture\n",
    "Incorporates both an encoder + decoder architecture\n",
    "\n",
    "The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.\n",
    "\n",
    "In the decoder, the self-attention layer only attends to earlier positions in the output sequence. The future positions are masked (setting them to -inf) before the softmax step in the self-attention calculation.\n",
    "\n",
    "The “Encoder-Decoder Attention” layer creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\n",
    "\n",
    "The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output.\n",
    "\n",
    "The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did.\n",
    "\n",
    "And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\n",
    "\n",
    "![Enc_Dec](images/transformer_decoding_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd61d79",
   "metadata": {},
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fab49b",
   "metadata": {
    "id": "XJ8TQcY0h1zD"
   },
   "source": [
    "### Encoder-only Transformers\n",
    "\n",
    "In addition to the encoder-decoder architecture shown here there various other architectures which are either only encoder or decoder models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca88f6f",
   "metadata": {
    "id": "QVEqV-ogiKlN"
   },
   "source": [
    "### Bidirectional Encoder Representations from Transformers (BERT) model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797d3f6",
   "metadata": {
    "id": "ykdG_rXojZx6"
   },
   "source": [
    "Encoder-only models only use the encoder layer of the Transformer.\n",
    "\n",
    "These models are usually used for \"understanding\" natural language; however, they typically are not used for text generation. Examples of uses for these models are:\n",
    "\n",
    "1. Determining how positive or negative a movie’s reviews are. (Sentiment Analysis)\n",
    "2. Summarizing long legal contracts. (Summarization)\n",
    "3. Differentiating words that have multiple meanings (like ‘bank’) based on the surrounding text. (Polysemy resolution)\n",
    "\n",
    "These models are often characterized as having “bi-directional” attention, and are often called auto-encoding models.\n",
    "The attention mechanisms of these models can access all the words in the initial sentence.\n",
    "\n",
    "The most common encoder only architectures are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c50e3",
   "metadata": {
    "id": "0CCuGvS4h9G4"
   },
   "source": [
    "* ALBERT\n",
    "* BERT\n",
    "* DistilBERT\n",
    "* ELECTRA\n",
    "* RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ed69a",
   "metadata": {
    "id": "yWQ2w9niYhpL"
   },
   "source": [
    "As example, let's consider BERT model in a little more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ddb478",
   "metadata": {
    "id": "_zqTajCjuMwz"
   },
   "source": [
    "![BERT_Explanation.webp](images/BERT_Explanation.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d6efa",
   "metadata": {
    "id": "wmOQzlfbbSRt"
   },
   "source": [
    "Image credit: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d461d",
   "metadata": {
    "id": "Jn4B4vvMlR0a"
   },
   "source": [
    "The BERT model is bidirectionally trained to have a deeper sense of language context and flow than single-direction language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93449edc",
   "metadata": {
    "id": "CQJ9gfGHmBwv"
   },
   "source": [
    "The Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35179e5a",
   "metadata": {
    "id": "4bGytwTrmZY8"
   },
   "source": [
    "In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaad3bf",
   "metadata": {
    "id": "pPlkDoWCnCPS"
   },
   "source": [
    "To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:\n",
    "\n",
    "1. A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n",
    "2. A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.\n",
    "3. A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b63a37",
   "metadata": {
    "id": "3vuT3OI2uUpc"
   },
   "source": [
    "![BERT_input_sent.webp](images/BERT_input_sent.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10928ae5",
   "metadata": {
    "id": "6i0EoRIDbVFE"
   },
   "source": [
    "Image credit: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25f0ce",
   "metadata": {
    "id": "bsMlclp_nVKV"
   },
   "source": [
    "To predict if the second sentence is indeed connected to the first, the following steps are performed:\n",
    "\n",
    "1. The entire input sequence goes through the Transformer model.\n",
    "2. The output of the [CLS] token is transformed into a 2×1 shaped vector, using a simple classification layer (learned matrices of weights and biases).\n",
    "3. Calculating the probability of IsNextSequence with softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6effd3",
   "metadata": {
    "id": "0OaH5uh5hOQE"
   },
   "source": [
    "#### Advantages and disadvantages:\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "* Contextualized embeddings: Good for tasks where contextualized embeddings of input tokens are crucial, such as natural language understanding.\n",
    "* Parallel processing: Allows for parallel processing of input tokens, making it computationally efficient.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* Not designed for sequence generation: Might not perform well on tasks that require sequential generation of output, as there is no inherent mechanism for auto-regressive decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24502cd8",
   "metadata": {
    "id": "sbSJ7_J9sK8Z"
   },
   "source": [
    "Here is an example of a BERT code that can be used to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6ff61b",
   "metadata": {
    "id": "Abjp33PHe3Ts"
   },
   "source": [
    "### Decoder-only models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53897281",
   "metadata": {
    "id": "IkIvScEFQSBg"
   },
   "source": [
    "An important difference of the GPT-2 architecture compared to the encoder-Transformer architecture has to do with the type of attention mechanism used.\n",
    "\n",
    "In models such as BERT, the self-attention mechanism has access to tokens to the left and right of the query token. However, in decoder-based models such as GPT-2, masked self-attention is used instead which allows access only to tokens to the left of the query.\n",
    "\n",
    "The masked self-attention mechanism is important for GPT-2 since it allows the model to be trained for token-by-token generation without simply \"memorizing\" the future tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9dc70",
   "metadata": {
    "id": "plPvVIOwunQW"
   },
   "source": [
    "![self-attention-and-masked-self-attention.png](images/self-attention-and-masked-self-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157849bd",
   "metadata": {
    "id": "PwvskuePcdRu"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6bfc0",
   "metadata": {
    "id": "Na3KlgI0Sae_"
   },
   "source": [
    "The masked self-attention adds understanding of associated words to explain contexts of certain words before passing it through a neural network. It assigns scores to how relevant each word in the segment is, and then adds up the vector representation. This is then passed through the feed-forward network resulting in an output vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd0fdf",
   "metadata": {
    "id": "sJShJx93u18W"
   },
   "source": [
    "![gpt2-self-attention-example-2.png](images/gpt2-self-attention-example-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd75bb",
   "metadata": {
    "id": "tDkNxx81cd0Q"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46efdf6",
   "metadata": {
    "id": "ZUMghdKOStK7"
   },
   "source": [
    "The resulting vector then needs to be converted to an output token. A common method of obtaining this output token is known as top-k.\n",
    "\n",
    "Here, the output vector is multiplied by the token embeddings which results in probabilities for each token in the vocabulary. Then the output token is sampled according to this probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be267d",
   "metadata": {
    "id": "YMkMD3jEu92U"
   },
   "source": [
    "![gpt2-output.png](images/gpt2-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b0c65",
   "metadata": {
    "id": "I1wNJMbJcfYM"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b3767",
   "metadata": {
    "id": "A-YdoSNljwpP"
   },
   "source": [
    "### Advantages and disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Auto-regressive generation: Well-suited for tasks that require sequential generation, as the model can generate one token at a time based on the previous tokens.\n",
    "* Variable-length output: Can handle tasks where the output sequence length is not fixed.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* No direct access to input context: The decoder doesn't directly consider the input context during decoding, which might be a limitation for certain tasks.\n",
    "* Potential for inefficiency: Decoding token by token can be less computationally efficient compared to parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed44e8d",
   "metadata": {
    "id": "HqXMaU4kVpWO"
   },
   "source": [
    "## Additional architectures\n",
    "\n",
    "In addition to text, LLMs have also been applied on other data sources such as images and graphs. Here I will describe two particular architectures:\n",
    "1. Vision Transformers\n",
    "2. Graph Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ab509",
   "metadata": {
    "id": "JW_Zl35qWIc5"
   },
   "source": [
    "### Vision Transformers\n",
    "\n",
    "Vision Transformers (ViT) is an architecture that uses self-attention mechanisms to process images.\n",
    "\n",
    "The way this works is:\n",
    "\n",
    "1. Split image into patches (size is fixed)\n",
    "2. Flatten the image patches\n",
    "3. Create lower-dimensional linear embeddings from these flattened image patches and include positional embeddings\n",
    "4. Feed the sequence as an input to a transformer encoder\n",
    "5. Pre-train the ViT model with image labels, which is then fully supervised on a big dataset Fine-tune the downstream dataset for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c06601",
   "metadata": {
    "id": "etRxFXTPvEjr"
   },
   "source": [
    "![vision-transformer-vit.png](images/vision-transformer-vit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99b8d4",
   "metadata": {
    "id": "gfbHrv7mW5jI"
   },
   "source": [
    "Image credit: Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995931c7",
   "metadata": {
    "id": "zw-WLqVrazU2"
   },
   "source": [
    "### Graph Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce6035",
   "metadata": {
    "id": "CYSCHt9SvNUJ"
   },
   "source": [
    "![Graphformer.png](images/Graphformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9168f1e3",
   "metadata": {
    "id": "nDXLTusqxXHf"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc293de",
   "metadata": {},
   "source": [
    "Here are some recommendations for further reading and additional code for review.\n",
    "\n",
    "* \"The Illustrated Transformer\" by Jay Alammar \n",
    "* \"Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)\"\n",
    "* \"The Illustrated GPT-2 (Visualizing Transformer Language Models)\"\n",
    "* \"A gentle introduction to positional encoding\"\n",
    "* \"LLM Tutorial Workshop (Argonne National Laboratory)\"\n",
    "* \"LLM Tutorial Workshop Part 2 (Argonne National Laboratory)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcbbecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
